---
title: More on ANOVA
output: 
  revealjs::revealjs_presentation:
    theme: night
    highlight: espresso
    center: true
    transition: none
    css: styles.css
    fig_caption: true
    reveal_options:
      progress: true
      slideNumber: true
      
  
---

## Linear Model Estimates

Consider a very simple one-sample linear model given by $y_i=\mu+\varepsilon_i$, $\varepsilon_i \sim N(0,\sigma^2)$ in matrix notation, given by $$\begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}=\begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix} \begin{pmatrix} \mu \end{pmatrix} + \begin{pmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{pmatrix}$$ with the vector $\varepsilon \sim N(0_{n \times 1},\sigma^2I_{n \times n})$.

##

Recalling that the normal distribution for one observation is given by $$\frac{1}{\sqrt{2 \pi}\sigma}\exp{-\frac{1}{2}(y_i-\mu)^2}$$ we can obtain the likelihood by taking the product over all $n$ independent observations:  \begin{eqnarray*}
L(y,\mu,\sigma)&=&\prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2}\frac{(y_i-\mu)^2}{\sigma^2}\right\} \\
&=& \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\mu)^2\right\}
\end{eqnarray*}

To find the MLE [solve for the parameter values that make the first derivative equal to 0](https://www.mathsisfun.com/calculus/maxima-minima.html) (often we work with the log-likelihood as it is more convenient).

##

The log-likelihood is given by

\begin{eqnarray*}
\ell(y,\mu,\sigma^2)&=& n \log \frac{1}{\sqrt(2\pi\sigma^2)} - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\mu)^2 \\
&=& -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\mu)^2
\end{eqnarray*}

##

To find the MLE of $\mu$, take the derivative

\begin{eqnarray*}
\frac{\partial \ell(\mu,\sigma^2)}{\partial \mu} &=& 0 -\frac{1}{2\sigma^2} \sum_{i=1}^n 2(y_i-\mu)(-1) \\
&=& \frac{1}{\sigma^2}\left(\sum_{i=1}^n y_i - n\mu \right)
\end{eqnarray*}

Setting this equal to zero, we obtain the MLE

\begin{eqnarray*}
n\widehat{\mu}&=&\sum_{i=1}^n y_i \\
\widehat{\mu}&=& \frac{\sum_{i=1}^n y_i}{n}=\overline{y}
\end{eqnarray*}

## 

To find the MLE of $\sigma^2$ take the derivative of $-\frac{n}{2}[\log(2\pi)+\log(\sigma^2)]- \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\mu)^2$

\begin{eqnarray*}
\frac{\partial \ell(\mu,\sigma^2)}{\partial \sigma^2} &=& 0-\frac{n}{2}\frac{1}{\sigma^2} - \frac{1}{2(-1)\left(\sigma^2\right)^2}\sum_{i=1}^n (y_i-\mu)^2 \\
&=& -\frac{n}{2\sigma^2} + \frac{1}{2\left(\sigma^2\right)^2}\sum_{i=1}^n (y_i-\mu)^2 
\end{eqnarray*}

Setting to 0 and solving for the MLE, using the MLE of $\mu$ we just found, we obtain 

$\widehat{\sigma}^2=\frac{1}{n}\sum_{i=1}^n (y_i-\overline{y})^2$.

Note this MLE of $\sigma^2$ is *not* the sample variance $s^2$. We will return to this point later in the course.





## Motivating Example: Cycling Safety

In the cycling safety study, after we found evidence that the rider's distance from the curb was related to passing distance (the overall F test), we wanted to learn what kind of relationship existed (pairwise comparisons). Each pairwise comparison is defined by a *linear combination* of the parameters in our model.

Consider the treatment means model $y_{ij}=\mu_j+\varepsilon_{ij}$. We are interested in which $\mu_j\neq\mu_j'$.

## Distribution of Least Squares Estimates

Recall in the linear model, the least squares estimate $\widehat{\beta}=(X'X)^{-1}X'y$. 

Its covariance is given by $\text{Cov}(\widehat{\beta})=\sigma^2(X'X)^{-1}$. 

In large samples, or when our errors are exactly normal, $\widehat{\beta} \sim N\left(\beta,\sigma^2(X'X)^{-1}\right)$.

## Linear Combinations

In order to test whether the means in group 1 and 2 are the same, we need to test a hypothesis about a *linear combination* of parameters.  

The quantity $\sum_j a_j \mu_j$ is a *linear combination*. It is called a *contrast* if $\sum_j a_j=0$. Using matrix notation, we often express a hypothesis regarding a linear combination of regression coefficients as

\begin{eqnarray*}
H_0: &\theta& = C\beta = \theta_0 \\
H_a: &\theta& = C\beta \neq \theta_0,
\end{eqnarray*}

where often $\theta_0=0$.

##

For example, if we have three groups and want to test differences in all pairwise comparisons, we can use $\beta=\begin{pmatrix} \mu_1 \\ \mu_2 \\ \mu_3 \end{pmatrix}$, $C=\begin{pmatrix} 1 & -1 & 0 \\ 1 & 0 & -1 \\ 0 & 1 & -1 \end{pmatrix}$, $\theta_0=\begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}$ so that our hypothesis is that $\begin{pmatrix} \mu_1 - \mu_2 \\ \mu_1 - \mu_3 \\ \mu_2 - \mu_3 \end{pmatrix}=\begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}$.

## Distributional Results for Linear Combinations

Using basic properties of the multivariate normal distribution, $$C \widehat{\beta} \sim N\left(C\beta,\sigma^2 C(X'X)^{-1}C'\right).$$

Using this result, you can derive the standard error for any linear combination of parameter estimates.

## Multi-Way ANOVA and Interactions

ANOVA is easily extended to accommodate any number of categorical variables. Variables may each contribute independently to a response, or they may work together to influence response values.

*Interaction effects* are important when the association between one independent variable and the response may depend on the level of another independent variable. 
[Click this link for nice insight on what interactions imply in terms of group means](https://courses.washington.edu/smartpsy/interactions.htm)

## Interaction Example

For example, suppose that we are interested in a two-way ANOVA model in which the response $y$ is a measure of headache pain, and the independent variables include the type of pill taken (placebo (j=1) or ibuprofen (j=2)) and the number of pills taken (k=1 or k=2). While we may expect lower pain if multiple ibuprofen pills are taken, we would not expect the same decrease in pain if multiple placebo pills were taken. 


Consider the model $y_{ijk}=\mu + \alpha I(j=2) + \beta I(k=2) + \gamma I(j=k=2)+\varepsilon_{ijk}$.

##

$y_{ijk}=\mu + \alpha I(j=2) + \beta I(k=2) + \gamma I(j=k=2)+\varepsilon_{ijk}$

In this model, the mean is parameterized as follows.

| Drug | \# of Pills | Mean |
| ---- | :-----------: | :----: |
| Placebo | 1 | $\mu$ | 
| Placebo | 2 | $\mu+\beta$ |
| Ibuprofen | 1 | $\mu+\alpha$ |
| Ibuprofen | 2 | $\mu +\alpha+\beta+\gamma$ |

  
##

$y_{ijk}=\mu + \alpha I(j=2) + \beta I(k=2) + \gamma I(j=k=2)+\varepsilon_{ijk}$

In this model,

  - the expected difference in pain level moving from 1 to 2 ibuprofen pills is $\mu+\alpha - \mu - \alpha - \beta - \gamma$
  - the expected difference in pain level moving from 1 to 2 placebo pills is $\mu - \mu - \beta$
  - the expected drug effect for those taking one pill is $\mu+\alpha-\mu=\alpha$
  - the expected drug effect for those taking two pills is $\mu+\alpha+\beta+\gamma - \mu - \beta=\alpha+\gamma$
  
  
So no interaction ($\gamma=0$) means that the drug effect is the same regardless of the number of pills taken. For there to be no drug effect at all, we need $\gamma=0$ and $\alpha=0$.  
  


## In class activity 

with some ANOVA data -- maybe some pharma data or the AB testing data that I used in STA 440 last fall?  Hmm. Use homework rubric to help them write up results.  USE THE BIOS600 PET FRIEND DATA EXAMPLE -- or is that only oneway?  ONLY ONEWAY IN DATA I HAVE

ideally use data that would be better suited for random effects ANOVA so that is a nice segway into the next lecture

GOT data?

Y=screen time
X1=gender
X2=season

